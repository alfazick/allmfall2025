{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2494b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a987f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_C.abi3.so\t\t       device_allocator    py.typed\n",
      "__init__.py\t\t       distributed\t   ray\n",
      "__pycache__\t\t       engine\t\t   reasoning\n",
      "_aiter_ops.py\t\t       entrypoints\t   sampling_params.py\n",
      "_bc_linter.py\t\t       env_override.py\t   scalar_type.py\n",
      "_custom_ops.py\t\t       envs.py\t\t   scripts.py\n",
      "_flashmla_C.abi3.so\t       forward_context.py  sequence.py\n",
      "_flashmla_extension_C.abi3.so  inputs\t\t   tasks.py\n",
      "_ipex_ops.py\t\t       logger.py\t   third_party\n",
      "_moe_C.abi3.so\t\t       logging_utils\t   tokenizers\n",
      "_version.py\t\t       logits_process.py   tracing.py\n",
      "assets\t\t\t       logprobs.py\t   transformers_utils\n",
      "attention\t\t       lora\t\t   triton_utils\n",
      "beam_search.py\t\t       model_executor\t   usage\n",
      "benchmarks\t\t       multimodal\t   utils\n",
      "collect_env.py\t\t       outputs.py\t   v1\n",
      "compilation\t\t       platforms\t   version.py\n",
      "config\t\t\t       plugins\t\t   vllm_flash_attn\n",
      "connections.py\t\t       pooling_params.py\n",
      "cumem_allocator.abi3.so        profiler\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e5d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py\t\t kv_cache_interface.py\tserial_utils.py\n",
      "__pycache__\t\t kv_offload\t\tspec_decode\n",
      "attention\t\t metrics\t\tstructured_output\n",
      "core\t\t\t outputs.py\t\tutils.py\n",
      "cudagraph_dispatcher.py  pool\t\t\tworker\n",
      "engine\t\t\t request.py\n",
      "executor\t\t sample\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789769c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py\t\t\t    kv_connector_model_runner_mixin.py\n",
      "__pycache__\t\t\t    lora_model_runner_mixin.py\n",
      "block_table.py\t\t\t    tpu_input_batch.py\n",
      "cpu_model_runner.py\t\t    tpu_model_runner.py\n",
      "cpu_worker.py\t\t\t    tpu_worker.py\n",
      "dp_utils.py\t\t\t    ubatch_utils.py\n",
      "ec_connector_model_runner_mixin.py  ubatching.py\n",
      "gpu\t\t\t\t    utils.py\n",
      "gpu_input_batch.py\t\t    worker_base.py\n",
      "gpu_model_runner.py\t\t    xpu_model_runner.py\n",
      "gpu_ubatch_wrapper.py\t\t    xpu_worker.py\n",
      "gpu_worker.py\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/v1/worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fa00b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-07 03:27:16 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 12-07 03:27:16 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 12-07 03:27:16 [interface.py:221] Failed to import from vllm._C: ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "GPUModelRunner.execute_model(\n",
      "    self,\n",
      "    scheduler_output: \u001b[33m'SchedulerOutput'\u001b[39m,\n",
      "    intermediate_tensors: vllm.sequence.IntermediateTensors | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> vllm.v1.outputs.ModelRunnerOutput | vllm.sequence.IntermediateTensors | \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m <no docstring>\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "    @torch.inference_mode()\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m execute_model(\n",
      "        self,\n",
      "        scheduler_output: \u001b[33m\"SchedulerOutput\"\u001b[39m,\n",
      "        intermediate_tensors: IntermediateTensors | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ) -> ModelRunnerOutput | IntermediateTensors | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.execute_model_state \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\n",
      "                \u001b[33m\"State error: sample_tokens() must be called \"\u001b[39m\n",
      "                \u001b[33m\"after execute_model() returns None.\"\u001b[39m\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;66;03m# self._draft_token_ids is None when `input_fits_in_drafter=False`\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# and there is no draft tokens scheduled. so it need to update the\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# spec_decoding info in scheduler_output with async_scheduling.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# use deepcopy to avoid the modification has influence on the\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# scheduler_output in engine core process.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# TODO(Ronald1995): deepcopy is expensive when there is a large\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# number of requests, optimize it later.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "            self.use_async_scheduling\n",
      "            \u001b[38;5;28;01mand\u001b[39;00m self.num_spec_tokens\n",
      "            \u001b[38;5;28;01mand\u001b[39;00m self._draft_token_ids \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        ):\n",
      "            scheduler_output = deepcopy(scheduler_output)\n",
      "\n",
      "        num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m record_function_or_nullcontext(\u001b[33m\"gpu_model_runner: preprocess\"\u001b[39m):\n",
      "            \u001b[38;5;28;01mwith\u001b[39;00m self.synchronize_input_prep():\n",
      "                \u001b[38;5;66;03m# Update persistent batch states.\u001b[39;00m\n",
      "                self._update_states(scheduler_output)\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m has_ec_transfer() \u001b[38;5;28;01mand\u001b[39;00m get_ec_transfer().is_producer:\n",
      "                    \u001b[38;5;28;01mwith\u001b[39;00m self.maybe_get_ec_connector_output(\n",
      "                        scheduler_output,\n",
      "                        encoder_cache=self.encoder_cache,\n",
      "                    ) \u001b[38;5;28;01mas\u001b[39;00m ec_connector_output:\n",
      "                        self._execute_mm_encoder(scheduler_output)\n",
      "                        \u001b[38;5;28;01mreturn\u001b[39;00m make_empty_encoder_model_runner_output(scheduler_output)\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m num_scheduled_tokens:\n",
      "                    \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                        self.parallel_config.distributed_executor_backend\n",
      "                        == \u001b[33m\"external_launcher\"\u001b[39m\n",
      "                        \u001b[38;5;28;01mand\u001b[39;00m self.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n",
      "                    ):\n",
      "                        \u001b[38;5;66;03m# this is a corner case when both external launcher\u001b[39;00m\n",
      "                        \u001b[38;5;66;03m# and DP are enabled, num_scheduled_tokens could be\u001b[39;00m\n",
      "                        \u001b[38;5;66;03m# 0, and has_unfinished_requests in the outer loop\u001b[39;00m\n",
      "                        \u001b[38;5;66;03m# returns True. before returning early here we call\u001b[39;00m\n",
      "                        \u001b[38;5;66;03m# dummy run to ensure coordinate_batch_across_dp\u001b[39;00m\n",
      "                        \u001b[38;5;66;03m# is called into to avoid out of sync issues.\u001b[39;00m\n",
      "                        self._dummy_run(\u001b[32m1\u001b[39m)\n",
      "                    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m has_kv_transfer_group():\n",
      "                        \u001b[38;5;66;03m# Return empty ModelRunnerOutput if no work to do.\u001b[39;00m\n",
      "                        \u001b[38;5;28;01mreturn\u001b[39;00m EMPTY_MODEL_RUNNER_OUTPUT\n",
      "                    \u001b[38;5;28;01mreturn\u001b[39;00m self.kv_connector_no_forward(\n",
      "                        scheduler_output, self.vllm_config\n",
      "                    )\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self.cache_config.kv_sharing_fast_prefill:\n",
      "                    \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.num_prompt_logprobs, (\n",
      "                        \u001b[33m\"--kv-sharing-fast-prefill produces incorrect \"\u001b[39m\n",
      "                        \u001b[33m\"logprobs for prompt tokens, tokens, please disable \"\u001b[39m\n",
      "                        \u001b[33m\"it when the requests need prompt logprobs\"\u001b[39m\n",
      "                    )\n",
      "\n",
      "                num_reqs = self.input_batch.num_reqs\n",
      "                req_ids = self.input_batch.req_ids\n",
      "                tokens = [scheduler_output.num_scheduled_tokens[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m req_ids]\n",
      "                num_scheduled_tokens_np = np.array(tokens, dtype=np.int32)\n",
      "                max_num_scheduled_tokens = int(num_scheduled_tokens_np.max())\n",
      "                num_tokens_unpadded = scheduler_output.total_num_scheduled_tokens\n",
      "\n",
      "                (\n",
      "                    logits_indices,\n",
      "                    spec_decode_metadata,\n",
      "                ) = self._prepare_inputs(\n",
      "                    scheduler_output,\n",
      "                    num_scheduled_tokens_np,\n",
      "                )\n",
      "\n",
      "                cascade_attn_prefix_lens = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# Disable cascade attention when using microbatching (DBO)\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self.cascade_attn_enabled \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.parallel_config.enable_dbo:\n",
      "                    \u001b[38;5;66;03m# Pre-compute cascade attention prefix lengths\u001b[39;00m\n",
      "                    cascade_attn_prefix_lens = self._compute_cascade_attn_prefix_lens(\n",
      "                        num_scheduled_tokens_np,\n",
      "                        self.input_batch.num_computed_tokens_cpu[:num_reqs],\n",
      "                        scheduler_output.num_common_prefix_blocks,\n",
      "                    )\n",
      "\n",
      "                (\n",
      "                    cudagraph_mode,\n",
      "                    batch_desc,\n",
      "                    ubatch_slices,\n",
      "                    num_tokens_across_dp,\n",
      "                ) = self._determine_batch_execution_and_padding(\n",
      "                    num_tokens=num_tokens_unpadded,\n",
      "                    num_reqs=num_reqs,\n",
      "                    num_scheduled_tokens_np=num_scheduled_tokens_np,\n",
      "                    max_num_scheduled_tokens=max_num_scheduled_tokens,\n",
      "                    use_cascade_attn=cascade_attn_prefix_lens \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                )\n",
      "\n",
      "                logger.debug(\n",
      "                    \u001b[33m\"Running batch with cudagraph_mode: %s, batch_descriptor: %s, \"\u001b[39m\n",
      "                    \u001b[33m\"ubatch_slices: %s, num_tokens_across_dp: %s\"\u001b[39m,\n",
      "                    cudagraph_mode,\n",
      "                    batch_desc,\n",
      "                    ubatch_slices,\n",
      "                    num_tokens_across_dp,\n",
      "                )\n",
      "\n",
      "                num_tokens_padded = batch_desc.num_tokens\n",
      "                num_reqs_padded = (\n",
      "                    batch_desc.num_reqs \u001b[38;5;28;01mif\u001b[39;00m batch_desc.num_reqs \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_reqs\n",
      "                )\n",
      "\n",
      "                use_spec_decode = len(scheduler_output.scheduled_spec_decode_tokens) > \u001b[32m0\u001b[39m\n",
      "                pad_attn = cudagraph_mode == CUDAGraphMode.FULL\n",
      "\n",
      "                (attn_metadata, spec_decode_common_attn_metadata) = (\n",
      "                    self._build_attention_metadata(\n",
      "                        num_tokens=num_tokens_unpadded,\n",
      "                        num_tokens_padded=num_tokens_padded \u001b[38;5;28;01mif\u001b[39;00m pad_attn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                        num_reqs=num_reqs,\n",
      "                        num_reqs_padded=num_reqs_padded \u001b[38;5;28;01mif\u001b[39;00m pad_attn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                        max_query_len=max_num_scheduled_tokens,\n",
      "                        ubatch_slices=ubatch_slices,\n",
      "                        logits_indices=logits_indices,\n",
      "                        use_spec_decode=use_spec_decode,\n",
      "                        num_scheduled_tokens=scheduler_output.num_scheduled_tokens,\n",
      "                        cascade_attn_prefix_lens=cascade_attn_prefix_lens,\n",
      "                    )\n",
      "                )\n",
      "\n",
      "            (\n",
      "                input_ids,\n",
      "                inputs_embeds,\n",
      "                positions,\n",
      "                intermediate_tensors,\n",
      "                model_kwargs,\n",
      "                ec_connector_output,\n",
      "            ) = self._preprocess(\n",
      "                scheduler_output, num_tokens_padded, intermediate_tensors\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;66;03m# Set cudagraph mode to none if calc_kv_scales is true.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# KV scales calculation involves dynamic operations that are incompatible\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# with CUDA graph capture.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.calculate_kv_scales:\n",
      "            cudagraph_mode = CUDAGraphMode.NONE\n",
      "            \u001b[38;5;66;03m# Mark KV scales as calculated after the first forward pass\u001b[39;00m\n",
      "            self.calculate_kv_scales = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "        \u001b[38;5;66;03m# Run the model.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# Use persistent buffers for CUDA graphs.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m (\n",
      "            set_forward_context(\n",
      "                attn_metadata,\n",
      "                self.vllm_config,\n",
      "                num_tokens=num_tokens_padded,\n",
      "                num_tokens_across_dp=num_tokens_across_dp,\n",
      "                cudagraph_runtime_mode=cudagraph_mode,\n",
      "                batch_descriptor=batch_desc,\n",
      "                ubatch_slices=ubatch_slices,\n",
      "            ),\n",
      "            record_function_or_nullcontext(\u001b[33m\"gpu_model_runner: forward\"\u001b[39m),\n",
      "            self.maybe_get_kv_connector_output(scheduler_output) \u001b[38;5;28;01mas\u001b[39;00m kv_connector_output,\n",
      "        ):\n",
      "            model_output = self._model_forward(\n",
      "                input_ids=input_ids,\n",
      "                positions=positions,\n",
      "                intermediate_tensors=intermediate_tensors,\n",
      "                inputs_embeds=inputs_embeds,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m record_function_or_nullcontext(\u001b[33m\"gpu_model_runner: postprocess\"\u001b[39m):\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.use_aux_hidden_state_outputs:\n",
      "                \u001b[38;5;66;03m# True when EAGLE 3 is used.\u001b[39;00m\n",
      "                hidden_states, aux_hidden_states = model_output\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                \u001b[38;5;66;03m# Common case.\u001b[39;00m\n",
      "                hidden_states = model_output\n",
      "                aux_hidden_states = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.broadcast_pp_output:\n",
      "                \u001b[38;5;66;03m# Common case.\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n",
      "                    \u001b[38;5;66;03m# Return the intermediate tensors.\u001b[39;00m\n",
      "                    \u001b[38;5;28;01massert\u001b[39;00m isinstance(hidden_states, IntermediateTensors)\n",
      "                    hidden_states.kv_connector_output = kv_connector_output\n",
      "                    self.kv_connector_output = kv_connector_output\n",
      "                    \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self.is_pooling_model:\n",
      "                    \u001b[38;5;66;03m# Return the pooling output.\u001b[39;00m\n",
      "                    output = self._pool(\n",
      "                        hidden_states, num_scheduled_tokens, num_scheduled_tokens_np\n",
      "                    )\n",
      "                    output.kv_connector_output = kv_connector_output\n",
      "                    \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\n",
      "                sample_hidden_states = hidden_states[logits_indices]\n",
      "                logits = self.model.compute_logits(sample_hidden_states)\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                \u001b[38;5;66;03m# Rare case.\u001b[39;00m\n",
      "                \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.is_pooling_model\n",
      "\n",
      "                sample_hidden_states = hidden_states[logits_indices]\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n",
      "                    all_gather_tensors = {\n",
      "                        \u001b[33m\"residual\"\u001b[39m: \u001b[38;5;28;01mnot\u001b[39;00m is_residual_scattered_for_sp(\n",
      "                            self.vllm_config, num_tokens_padded\n",
      "                        )\n",
      "                    }\n",
      "                    get_pp_group().send_tensor_dict(\n",
      "                        hidden_states.tensors,\n",
      "                        all_gather_group=get_tp_group(),\n",
      "                        all_gather_tensors=all_gather_tensors,\n",
      "                    )\n",
      "                    logits = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                    logits = self.model.compute_logits(sample_hidden_states)\n",
      "\n",
      "                model_output_broadcast_data: dict[str, Any] = {}\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    model_output_broadcast_data[\u001b[33m\"logits\"\u001b[39m] = logits.contiguous()\n",
      "\n",
      "                broadcasted = get_pp_group().broadcast_tensor_dict(\n",
      "                    model_output_broadcast_data, src=len(get_pp_group().ranks) - \u001b[32m1\u001b[39m\n",
      "                )\n",
      "                \u001b[38;5;28;01massert\u001b[39;00m broadcasted \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                logits = broadcasted[\u001b[33m\"logits\"\u001b[39m]\n",
      "\n",
      "        self.execute_model_state = ExecuteModelState(\n",
      "            scheduler_output,\n",
      "            logits,\n",
      "            spec_decode_metadata,\n",
      "            spec_decode_common_attn_metadata,\n",
      "            hidden_states,\n",
      "            sample_hidden_states,\n",
      "            aux_hidden_states,\n",
      "            ec_connector_output,\n",
      "        )\n",
      "        self.kv_connector_output = kv_connector_output\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mFile:\u001b[39m      /usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "from vllm.v1.worker.gpu_model_runner import GPUModelRunner\n",
    "GPUModelRunner.execute_model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0062e371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m GPUModelRunner._update_states(self, scheduler_output: \u001b[33m'SchedulerOutput'\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _update_states(self, scheduler_output: \u001b[33m\"SchedulerOutput\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Update the cached states and the persistent batch with the scheduler\u001b[39m\n",
      "\u001b[33m        output.\u001b[39m\n",
      "\n",
      "\u001b[33m        The updated states are used by the `_prepare_inputs` function to create\u001b[39m\n",
      "\u001b[33m        the input GPU tensors for the model.\u001b[39m\n",
      "\n",
      "\u001b[33m        The SamplingMetadata is updated and copied to the GPU if there is a\u001b[39m\n",
      "\u001b[33m        new/resumed/paused/finished request in the batch.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;66;03m# Remove finished requests from the cached states.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m req_id \u001b[38;5;28;01min\u001b[39;00m scheduler_output.finished_req_ids:\n",
      "            self.requests.pop(req_id, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "            self.num_prompt_logprobs.pop(req_id, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "        \u001b[38;5;66;03m# Remove the finished requests from the persistent batch.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# NOTE(woosuk): There could be an edge case where finished_req_ids and\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# scheduled_req_ids overlap. This happens when a request is aborted and\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# then resubmitted with the same ID. In this case, we treat them as two\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# distinct requests - clearing the cached states for the first request\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# and handling the second as a new request.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m req_id \u001b[38;5;28;01min\u001b[39;00m scheduler_output.finished_req_ids:\n",
      "            self.input_batch.remove_request(req_id)\n",
      "\n",
      "        \u001b[38;5;66;03m# Free the cached encoder outputs.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m mm_hash \u001b[38;5;28;01min\u001b[39;00m scheduler_output.free_encoder_mm_hashes:\n",
      "            self.encoder_cache.pop(mm_hash, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\n",
      "        \u001b[38;5;66;03m# Remove the unscheduled requests from the persistent batch.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# NOTE(woosuk): The unscheduled requests are either preempted requests\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# or running requests that are not scheduled in this step. We remove\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# them from the persistent batch but keep their cached states since\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# they will be scheduled again sometime in the future.\u001b[39;00m\n",
      "        scheduled_req_ids = scheduler_output.num_scheduled_tokens.keys()\n",
      "        cached_req_ids = self.input_batch.req_id_to_index.keys()\n",
      "        resumed_req_ids = scheduler_output.scheduled_cached_reqs.resumed_req_ids\n",
      "        \u001b[38;5;66;03m# NOTE(zhuohan): cached_req_ids and resumed_req_ids are usually disjoint,\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# so `(scheduled_req_ids - resumed_req_ids) == scheduled_req_ids` holds\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# apart from the forced-preemption case in reset_prefix_cache. And in\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# that case we include the resumed_req_ids in the unscheduled set so\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# that they get cleared from the persistent batch before being re-scheduled\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# in the normal resumed request path.\u001b[39;00m\n",
      "        unscheduled_req_ids = cached_req_ids - (scheduled_req_ids - resumed_req_ids)\n",
      "        \u001b[38;5;66;03m# NOTE(woosuk): The persistent batch optimization assumes that\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# consecutive batches contain mostly the same requests. If batches\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# have low request overlap (e.g., alternating between two distinct\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# sets of requests), this optimization becomes very inefficient.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m req_id \u001b[38;5;28;01min\u001b[39;00m unscheduled_req_ids:\n",
      "            self.input_batch.remove_request(req_id)\n",
      "\n",
      "        reqs_to_add: list[CachedRequestState] = []\n",
      "        \u001b[38;5;66;03m# Add new requests to the cached states.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m new_req_data \u001b[38;5;28;01min\u001b[39;00m scheduler_output.scheduled_new_reqs:\n",
      "            req_id = new_req_data.req_id\n",
      "            sampling_params = new_req_data.sampling_params\n",
      "            pooling_params = new_req_data.pooling_params\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                sampling_params\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m sampling_params.sampling_type == SamplingType.RANDOM_SEED\n",
      "            ):\n",
      "                generator = torch.Generator(device=self.device)\n",
      "                generator.manual_seed(sampling_params.seed)\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                generator = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.is_pooling_model:\n",
      "                \u001b[38;5;28;01massert\u001b[39;00m pooling_params \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                task = pooling_params.task\n",
      "                \u001b[38;5;28;01massert\u001b[39;00m task \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"You did not set `task` in the API\"\u001b[39m\n",
      "\n",
      "                model = cast(VllmModelForPooling, self.get_model())\n",
      "                to_update = model.pooler.get_pooling_updates(task)\n",
      "                to_update.apply(pooling_params)\n",
      "\n",
      "            req_state = CachedRequestState(\n",
      "                req_id=req_id,\n",
      "                prompt_token_ids=new_req_data.prompt_token_ids,\n",
      "                prompt_embeds=new_req_data.prompt_embeds,\n",
      "                mm_features=new_req_data.mm_features,\n",
      "                sampling_params=sampling_params,\n",
      "                pooling_params=pooling_params,\n",
      "                generator=generator,\n",
      "                block_ids=new_req_data.block_ids,\n",
      "                num_computed_tokens=new_req_data.num_computed_tokens,\n",
      "                output_token_ids=[],\n",
      "                lora_request=new_req_data.lora_request,\n",
      "            )\n",
      "            self.requests[req_id] = req_state\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m sampling_params \u001b[38;5;28;01mand\u001b[39;00m sampling_params.prompt_logprobs \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                self.num_prompt_logprobs[req_id] = (\n",
      "                    self.input_batch.vocab_size\n",
      "                    \u001b[38;5;28;01mif\u001b[39;00m sampling_params.prompt_logprobs == -\u001b[32m1\u001b[39m\n",
      "                    \u001b[38;5;28;01melse\u001b[39;00m sampling_params.prompt_logprobs\n",
      "                )\n",
      "\n",
      "            \u001b[38;5;66;03m# Only relevant for models using M-RoPE (e.g, Qwen2-VL)\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.uses_mrope:\n",
      "                self._init_mrope_positions(req_state)\n",
      "\n",
      "            \u001b[38;5;66;03m# Only relevant for models using XD-RoPE (e.g, HunYuan-VL)\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.uses_xdrope_dim > \u001b[32m0\u001b[39m:\n",
      "                self._init_xdrope_positions(req_state)\n",
      "\n",
      "            reqs_to_add.append(req_state)\n",
      "\n",
      "        \u001b[38;5;66;03m# Update the states of the running/resumed requests.\u001b[39;00m\n",
      "        is_last_rank = get_pp_group().is_last_rank\n",
      "        req_data = scheduler_output.scheduled_cached_reqs\n",
      "\n",
      "        \u001b[38;5;66;03m# Wait until valid_sampled_tokens_count is copied to cpu,\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# then use it to update actual num_computed_tokens of each request.\u001b[39;00m\n",
      "        valid_sampled_token_count = self._get_valid_sampled_token_count()\n",
      "\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m i, req_id \u001b[38;5;28;01min\u001b[39;00m enumerate(req_data.req_ids):\n",
      "            req_state = self.requests[req_id]\n",
      "            num_computed_tokens = req_data.num_computed_tokens[i]\n",
      "            new_block_ids = req_data.new_block_ids[i]\n",
      "            resumed_from_preemption = req_id \u001b[38;5;28;01min\u001b[39;00m req_data.resumed_req_ids\n",
      "            num_output_tokens = req_data.num_output_tokens[i]\n",
      "            req_index = self.input_batch.req_id_to_index.get(req_id)\n",
      "\n",
      "            \u001b[38;5;66;03m# prev_num_draft_len is used in async scheduling mode with\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# spec decode. it indicates if need to update num_computed_tokens\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# of the request. for example:\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# fist step: num_computed_tokens = 0, spec_tokens = [],\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# prev_num_draft_len = 0.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# second step: num_computed_tokens = 100(prompt lenth),\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# spec_tokens = [a,b], prev_num_draft_len = 0.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# third step: num_computed_tokens = 100 + 2, spec_tokens = [c,d],\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# prev_num_draft_len = 2.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# num_computed_tokens in first step and second step does't contain\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# the spec tokens length, but in third step it contains the\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# spec tokens length. we only need to update num_computed_tokens\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# when prev_num_draft_len > 0.\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m req_state.prev_num_draft_len:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m req_index \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    req_state.prev_num_draft_len = \u001b[32m0\u001b[39m\n",
      "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                    \u001b[38;5;28;01massert\u001b[39;00m self.input_batch.prev_req_id_to_index \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                    prev_req_index = self.input_batch.prev_req_id_to_index[req_id]\n",
      "                    num_accepted = valid_sampled_token_count[prev_req_index] - \u001b[32m1\u001b[39m\n",
      "                    num_rejected = req_state.prev_num_draft_len - num_accepted\n",
      "                    num_computed_tokens -= num_rejected\n",
      "                    req_state.output_token_ids.extend([-\u001b[32m1\u001b[39m] * num_accepted)\n",
      "\n",
      "            \u001b[38;5;66;03m# Update the cached states.\u001b[39;00m\n",
      "            req_state.num_computed_tokens = num_computed_tokens\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_last_rank:\n",
      "                \u001b[38;5;66;03m# When using PP, the scheduler sends the sampled tokens back,\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# because there's no direct communication between the first-\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# stage worker and the last-stage worker.\u001b[39;00m\n",
      "                new_token_ids = req_data.new_token_ids[i]\n",
      "                \u001b[38;5;66;03m# Add the sampled token(s) from the previous step (if any).\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# This doesn't include \"unverified\" tokens like spec tokens.\u001b[39;00m\n",
      "                num_new_tokens = (\n",
      "                    num_computed_tokens + len(new_token_ids) - req_state.num_tokens\n",
      "                )\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m num_new_tokens == \u001b[32m1\u001b[39m:\n",
      "                    \u001b[38;5;66;03m# Avoid slicing list in most common case.\u001b[39;00m\n",
      "                    req_state.output_token_ids.append(new_token_ids[-\u001b[32m1\u001b[39m])\n",
      "                \u001b[38;5;28;01melif\u001b[39;00m num_new_tokens > \u001b[32m0\u001b[39m:\n",
      "                    req_state.output_token_ids.extend(new_token_ids[-num_new_tokens:])\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m num_output_tokens < len(req_state.output_token_ids):\n",
      "                \u001b[38;5;66;03m# Some output tokens were discarded due to a sync-KV-load\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# failure. Align the cached state.\u001b[39;00m\n",
      "                \u001b[38;5;28;01mdel\u001b[39;00m req_state.output_token_ids[num_output_tokens:]\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m req_index \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    end_idx = (\n",
      "                        self.input_batch.num_prompt_tokens[req_index]\n",
      "                        + num_output_tokens\n",
      "                    )\n",
      "                    self.input_batch.num_tokens[req_index] = end_idx\n",
      "                    self.input_batch.num_tokens_no_spec[req_index] = end_idx\n",
      "\n",
      "            \u001b[38;5;66;03m# Update the block IDs.\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m resumed_from_preemption:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m new_block_ids \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    \u001b[38;5;66;03m# Append the new blocks to the existing block IDs.\u001b[39;00m\n",
      "                    \u001b[38;5;28;01mfor\u001b[39;00m block_ids, new_ids \u001b[38;5;28;01min\u001b[39;00m zip(req_state.block_ids, new_block_ids):\n",
      "                        block_ids.extend(new_ids)\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                \u001b[38;5;28;01massert\u001b[39;00m req_index \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                \u001b[38;5;28;01massert\u001b[39;00m new_block_ids \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# The request is resumed from preemption.\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# Replace the existing block IDs with the new ones.\u001b[39;00m\n",
      "                req_state.block_ids = new_block_ids\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m req_index \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                \u001b[38;5;66;03m# The request is not in the persistent batch.\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# The request was either preempted and resumed later, or was not\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# scheduled in the previous step and needs to be added again.\u001b[39;00m\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self.use_async_scheduling \u001b[38;5;28;01mand\u001b[39;00m num_output_tokens > \u001b[32m0\u001b[39m:\n",
      "                    \u001b[38;5;66;03m# We must recover the output token ids for resumed requests in the\u001b[39;00m\n",
      "                    \u001b[38;5;66;03m# async scheduling case, so that correct input_ids are obtained.\u001b[39;00m\n",
      "                    resumed_token_ids = req_data.all_token_ids[req_id]\n",
      "                    req_state.output_token_ids = resumed_token_ids[-num_output_tokens:]\n",
      "\n",
      "                reqs_to_add.append(req_state)\n",
      "                \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\n",
      "            \u001b[38;5;66;03m# Update the persistent batch.\u001b[39;00m\n",
      "            self.input_batch.num_computed_tokens_cpu[req_index] = num_computed_tokens\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m new_block_ids \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                self.input_batch.block_table.append_row(new_block_ids, req_index)\n",
      "\n",
      "            \u001b[38;5;66;03m# For the last rank, we don't need to update the token_ids_cpu\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# because the sampled tokens are already cached.\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_last_rank:\n",
      "                \u001b[38;5;66;03m# Add new_token_ids to token_ids_cpu.\u001b[39;00m\n",
      "                start_token_index = num_computed_tokens\n",
      "                end_token_index = num_computed_tokens + len(new_token_ids)\n",
      "                self.input_batch.token_ids_cpu[\n",
      "                    req_index, start_token_index:end_token_index\n",
      "                ] = new_token_ids\n",
      "                self.input_batch.num_tokens_no_spec[req_index] = end_token_index\n",
      "                self.input_batch.num_tokens[req_index] = end_token_index\n",
      "\n",
      "            \u001b[38;5;66;03m# Add spec_token_ids to token_ids_cpu.\u001b[39;00m\n",
      "            spec_token_ids = scheduler_output.scheduled_spec_decode_tokens.get(\n",
      "                req_id, []\n",
      "            )\n",
      "            num_spec_tokens = len(spec_token_ids)\n",
      "            \u001b[38;5;66;03m# For async scheduling, token_ids_cpu assigned from\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# spec_token_ids are placeholders and will be overwritten in\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# _prepare_input_ids.\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m num_spec_tokens:\n",
      "                start_index = self.input_batch.num_tokens_no_spec[req_index]\n",
      "                end_token_index = start_index + num_spec_tokens\n",
      "                self.input_batch.token_ids_cpu[\n",
      "                    req_index, start_index:end_token_index\n",
      "                ] = spec_token_ids\n",
      "                \u001b[38;5;66;03m# NOTE(woosuk): `num_tokens` here may include spec tokens.\u001b[39;00m\n",
      "                self.input_batch.num_tokens[req_index] += num_spec_tokens\n",
      "\n",
      "            \u001b[38;5;66;03m# When speculative decoding is used with structured output,\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# the scheduler can drop draft tokens that do not\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# conform to the schema. This can result in\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# scheduler_output.scheduled_spec_decode_tokens being empty,\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# even when speculative decoding is enabled.\u001b[39;00m\n",
      "            self.input_batch.spec_token_ids[req_index].clear()\n",
      "            self.input_batch.spec_token_ids[req_index].extend(spec_token_ids)\n",
      "\n",
      "            \u001b[38;5;66;03m# there are no draft tokens with async scheduling,\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# we clear the spec_decoding info in scheduler_output and\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# use normal sampling but rejection_sampling.\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.use_async_scheduling:\n",
      "                req_state.prev_num_draft_len = num_spec_tokens\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m num_spec_tokens \u001b[38;5;28;01mand\u001b[39;00m self._draft_token_ids \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    scheduler_output.total_num_scheduled_tokens -= num_spec_tokens\n",
      "                    scheduler_output.num_scheduled_tokens[req_id] -= num_spec_tokens\n",
      "                    scheduler_output.scheduled_spec_decode_tokens.pop(req_id, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "        \u001b[38;5;66;03m# Add the new or resumed requests to the persistent batch.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# The smaller empty indices are filled first.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m request \u001b[38;5;28;01min\u001b[39;00m reqs_to_add:\n",
      "            self.input_batch.add_request(request)\n",
      "\n",
      "        \u001b[38;5;66;03m# Condense the batched states if there are gaps left by removed requests\u001b[39;00m\n",
      "        self.input_batch.condense()\n",
      "        \u001b[38;5;66;03m# Allow attention backend to reorder the batch, potentially\u001b[39;00m\n",
      "        self._may_reorder_batch(scheduler_output)\n",
      "        \u001b[38;5;66;03m# Refresh batch metadata with any pending updates.\u001b[39;00m\n",
      "        self.input_batch.refresh_metadata()\n",
      "\u001b[31mFile:\u001b[39m      /usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "GPUModelRunner._update_states??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aaaf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
