{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f985d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-07 16:12:25 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 12-07 16:12:25 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 12-07 16:12:25 [interface.py:221] Failed to import from vllm._C: ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "GPUModelRunner._prepare_inputs(\n",
      "    self,\n",
      "    scheduler_output: \u001b[33m'SchedulerOutput'\u001b[39m,\n",
      "    num_scheduled_tokens: numpy.ndarray,\n",
      ") -> tuple[torch.Tensor, vllm.v1.spec_decode.metadata.SpecDecodeMetadata | \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _prepare_inputs(\n",
      "        self,\n",
      "        scheduler_output: \u001b[33m\"SchedulerOutput\"\u001b[39m,\n",
      "        num_scheduled_tokens: np.ndarray,\n",
      "    ) -> tuple[\n",
      "        torch.Tensor,\n",
      "        SpecDecodeMetadata | \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ]:\n",
      "        \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m        :return: tuple[\u001b[39m\n",
      "\u001b[33m            logits_indices, spec_decode_metadata,\u001b[39m\n",
      "\u001b[33m        ]\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n",
      "        \u001b[38;5;28;01massert\u001b[39;00m total_num_scheduled_tokens > \u001b[32m0\u001b[39m\n",
      "        num_reqs = self.input_batch.num_reqs\n",
      "        \u001b[38;5;28;01massert\u001b[39;00m num_reqs > \u001b[32m0\u001b[39m\n",
      "\n",
      "        \u001b[38;5;66;03m# OPTIMIZATION: Start copying the block table first.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# This way, we can overlap the copy with the following CPU operations.\u001b[39;00m\n",
      "        self.input_batch.block_table.commit_block_table(num_reqs)\n",
      "\n",
      "        \u001b[38;5;66;03m# Get request indices.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\u001b[39;00m\n",
      "        req_indices = np.repeat(self.arange_np[:num_reqs], num_scheduled_tokens)\n",
      "\n",
      "        \u001b[38;5;66;03m# cu_num_tokens: [2, 5, 3] -> [2, 7, 10]\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# arange: [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\u001b[39;00m\n",
      "        cu_num_tokens, arange = self._get_cumsum_and_arange(num_scheduled_tokens)\n",
      "\n",
      "        \u001b[38;5;66;03m# Get positions.\u001b[39;00m\n",
      "        positions_np = self.positions.np[:total_num_scheduled_tokens]\n",
      "        np.add(\n",
      "            self.input_batch.num_computed_tokens_cpu[req_indices],\n",
      "            arange,\n",
      "            out=positions_np,\n",
      "        )\n",
      "\n",
      "        \u001b[38;5;66;03m# Calculate M-RoPE positions.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# Only relevant for models using M-RoPE (e.g, Qwen2-VL)\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.uses_mrope:\n",
      "            self._calc_mrope_positions(scheduler_output)\n",
      "\n",
      "        \u001b[38;5;66;03m# Calculate XD-RoPE positions.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# Only relevant for models using XD-RoPE (e.g, HunYuan-VL)\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.uses_xdrope_dim > \u001b[32m0\u001b[39m:\n",
      "            self._calc_xdrope_positions(scheduler_output)\n",
      "\n",
      "        \u001b[38;5;66;03m# Get token indices.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# -> [0, 1, M, M + 1, M + 2, M + 3, M + 4, 2 * M, 2 * M + 1, 2 * M + 2]\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# where M is the max_model_len.\u001b[39;00m\n",
      "        token_indices = (\n",
      "            positions_np + req_indices * self.input_batch.token_ids_cpu.shape[\u001b[32m1\u001b[39m]\n",
      "        )\n",
      "        token_indices_tensor = torch.from_numpy(token_indices)\n",
      "\n",
      "        \u001b[38;5;66;03m# NOTE(woosuk): We use torch.index_select instead of np.take here\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# because torch.index_select is much faster than np.take for large\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# tensors.\u001b[39;00m\n",
      "        torch.index_select(\n",
      "            self.input_batch.token_ids_cpu_tensor.flatten(),\n",
      "            \u001b[32m0\u001b[39m,\n",
      "            token_indices_tensor,\n",
      "            out=self.input_ids.cpu[:total_num_scheduled_tokens],\n",
      "        )\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.enable_prompt_embeds:\n",
      "            is_token_ids = self.input_batch.is_token_ids_tensor.flatten()\n",
      "            torch.index_select(\n",
      "                is_token_ids,\n",
      "                \u001b[32m0\u001b[39m,\n",
      "                token_indices_tensor,\n",
      "                out=self.is_token_ids.cpu[:total_num_scheduled_tokens],\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;66;03m# Because we did not pre-allocate a massive prompt_embeds CPU tensor on\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# the InputBatch, we need to fill in the prompt embeds into the expected\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# spots in the GpuModelRunner's pre-allocated prompt_embeds tensor.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.input_batch.req_prompt_embeds:\n",
      "            output_idx = \u001b[32m0\u001b[39m\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m req_idx \u001b[38;5;28;01min\u001b[39;00m range(num_reqs):\n",
      "                num_sched = num_scheduled_tokens[req_idx]\n",
      "\n",
      "                \u001b[38;5;66;03m# Skip if this request doesn't have embeddings\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m req_idx \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.input_batch.req_prompt_embeds:\n",
      "                    output_idx += num_sched\n",
      "                    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\n",
      "                \u001b[38;5;66;03m# Skip if no tokens scheduled\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m num_sched <= \u001b[32m0\u001b[39m:\n",
      "                    output_idx += num_sched\n",
      "                    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\n",
      "                req_embeds = self.input_batch.req_prompt_embeds[req_idx]\n",
      "                start_pos = self.input_batch.num_computed_tokens_cpu[req_idx]\n",
      "\n",
      "                \u001b[38;5;66;03m# Skip if trying to read beyond available embeddings\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m start_pos >= req_embeds.shape[\u001b[32m0\u001b[39m]:\n",
      "                    output_idx += num_sched\n",
      "                    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\n",
      "                \u001b[38;5;66;03m# Copy available embeddings\u001b[39;00m\n",
      "                end_pos = start_pos + num_sched\n",
      "                actual_end = min(end_pos, req_embeds.shape[\u001b[32m0\u001b[39m])\n",
      "                actual_num_sched = actual_end - start_pos\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m actual_num_sched > \u001b[32m0\u001b[39m:\n",
      "                    self.inputs_embeds.cpu[\n",
      "                        output_idx : output_idx + actual_num_sched\n",
      "                    ].copy_(req_embeds[start_pos:actual_end])\n",
      "\n",
      "                output_idx += num_sched\n",
      "\n",
      "        self.input_batch.block_table.compute_slot_mapping(req_indices, positions_np)\n",
      "        self.input_batch.block_table.commit_slot_mapping(total_num_scheduled_tokens)\n",
      "\n",
      "        \u001b[38;5;66;03m# Prepare the attention metadata.\u001b[39;00m\n",
      "        self.query_start_loc.np[\u001b[32m0\u001b[39m] = \u001b[32m0\u001b[39m\n",
      "        self.query_start_loc.np[\u001b[32m1\u001b[39m : num_reqs + \u001b[32m1\u001b[39m] = cu_num_tokens\n",
      "        \u001b[38;5;66;03m# Note: pad query_start_loc to be non-decreasing, as kernels\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# like FlashAttention requires that\u001b[39;00m\n",
      "        self.query_start_loc.np[num_reqs + \u001b[32m1\u001b[39m :].fill(cu_num_tokens[-\u001b[32m1\u001b[39m])\n",
      "        self.query_start_loc.copy_to_gpu()\n",
      "        query_start_loc = self.query_start_loc.gpu[: num_reqs + \u001b[32m1\u001b[39m]\n",
      "\n",
      "        self.seq_lens.np[:num_reqs] = (\n",
      "            self.input_batch.num_computed_tokens_cpu[:num_reqs] + num_scheduled_tokens\n",
      "        )\n",
      "        \u001b[38;5;66;03m# Fill unused with 0 for full cuda graph mode.\u001b[39;00m\n",
      "        self.seq_lens.np[num_reqs:].fill(\u001b[32m0\u001b[39m)\n",
      "        self.seq_lens.copy_to_gpu()\n",
      "\n",
      "        num_tokens = [self.requests[r].num_tokens \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;28;01min\u001b[39;00m self.input_batch.req_ids]\n",
      "        num_tokens_np = np.array(num_tokens, dtype=np.int32)\n",
      "\n",
      "        \u001b[38;5;66;03m# Record which requests should not be sampled,\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# so that we could clear the sampled tokens before returning\u001b[39;00m\n",
      "        self.discard_request_mask.np[:num_reqs] = (\n",
      "            self.seq_lens.np[:num_reqs] < num_tokens_np\n",
      "        )\n",
      "        self.discard_request_mask.copy_to_gpu(num_reqs)\n",
      "\n",
      "        \u001b[38;5;66;03m# Copy the tensors to the GPU.\u001b[39;00m\n",
      "        self._prepare_input_ids(\n",
      "            scheduler_output,\n",
      "            total_num_scheduled_tokens,\n",
      "            cu_num_tokens,\n",
      "        )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.uses_mrope:\n",
      "            \u001b[38;5;66;03m# Only relevant for models using M-RoPE (e.g, Qwen2-VL)\u001b[39;00m\n",
      "            self.mrope_positions.gpu[:, :total_num_scheduled_tokens].copy_(\n",
      "                self.mrope_positions.cpu[:, :total_num_scheduled_tokens],\n",
      "                non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "            )\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.uses_xdrope_dim > \u001b[32m0\u001b[39m:\n",
      "            \u001b[38;5;66;03m# Only relevant for models using XD-RoPE (e.g, HunYuan-VL)\u001b[39;00m\n",
      "            self.xdrope_positions.gpu[:, :total_num_scheduled_tokens].copy_(\n",
      "                self.xdrope_positions.cpu[:, :total_num_scheduled_tokens],\n",
      "                non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "            )\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# Common case (1D positions)\u001b[39;00m\n",
      "            self.positions.copy_to_gpu(total_num_scheduled_tokens)\n",
      "\n",
      "        use_spec_decode = len(scheduler_output.scheduled_spec_decode_tokens) > \u001b[32m0\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m use_spec_decode:\n",
      "            \u001b[38;5;66;03m# NOTE(woosuk): Due to chunked prefills, the batch may contain\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# partial requests. While we should not sample any token\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# from these partial requests, we do so for simplicity.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# We will ignore the sampled tokens from the partial requests.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# TODO: Support prompt logprobs.\u001b[39;00m\n",
      "            logits_indices = query_start_loc[\u001b[32m1\u001b[39m:] - \u001b[32m1\u001b[39m\n",
      "            num_draft_tokens = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "            spec_decode_metadata = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "            num_sampled_tokens = np.ones(num_reqs, dtype=np.int32)\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# Get the number of draft tokens for each request.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# Iterate over the dictionary rather than all requests since not all\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# requests have draft tokens.\u001b[39;00m\n",
      "            num_draft_tokens = np.zeros(num_reqs, dtype=np.int32)\n",
      "            \u001b[38;5;66;03m# For chunked prefills, use -1 as mask rather than 0, as guided\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# decoding may rollback speculative tokens.\u001b[39;00m\n",
      "            num_decode_draft_tokens = np.full(num_reqs, -\u001b[32m1\u001b[39m, dtype=np.int32)\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m (\n",
      "                req_id,\n",
      "                draft_token_ids,\n",
      "            ) \u001b[38;5;28;01min\u001b[39;00m scheduler_output.scheduled_spec_decode_tokens.items():\n",
      "                req_idx = self.input_batch.req_id_to_index[req_id]\n",
      "                num_draft_tokens[req_idx] = len(draft_token_ids)\n",
      "                num_decode_draft_tokens[req_idx] = (\n",
      "                    len(draft_token_ids)\n",
      "                    \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                        self.input_batch.num_computed_tokens_cpu[req_idx]\n",
      "                        >= self.input_batch.num_prompt_tokens[req_idx]\n",
      "                    )\n",
      "                    \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m\n",
      "                )\n",
      "            spec_decode_metadata = self._calc_spec_decode_metadata(\n",
      "                num_draft_tokens, cu_num_tokens\n",
      "            )\n",
      "            logits_indices = spec_decode_metadata.logits_indices\n",
      "            num_sampled_tokens = num_draft_tokens + \u001b[32m1\u001b[39m\n",
      "            \u001b[38;5;66;03m# For DECODE only cuda graph of some attention backends (e.g., GDN).\u001b[39;00m\n",
      "            self.num_decode_draft_tokens.np[:num_reqs] = num_decode_draft_tokens\n",
      "            self.num_decode_draft_tokens.np[num_reqs:].fill(-\u001b[32m1\u001b[39m)\n",
      "            self.num_decode_draft_tokens.copy_to_gpu()\n",
      "\n",
      "        \u001b[38;5;66;03m# Hot-Swap lora model\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.lora_config:\n",
      "            \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "                np.sum(num_sampled_tokens)\n",
      "                <= self.vllm_config.scheduler_config.max_num_batched_tokens\n",
      "            )\n",
      "            self.set_active_loras(\n",
      "                self.input_batch, num_scheduled_tokens, num_sampled_tokens\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m (\n",
      "            logits_indices,\n",
      "            spec_decode_metadata,\n",
      "        )\n",
      "\u001b[31mFile:\u001b[39m      /usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "from vllm.v1.worker.gpu_model_runner import GPUModelRunner\n",
    "GPUModelRunner._prepare_inputs??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea29d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
