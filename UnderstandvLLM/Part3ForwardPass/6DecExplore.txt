I want to trace how execution happens

so looks like everything happens here
!ls /usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/v1/worker

from vllm.v1.worker.gpu_model_runner import GPUModelRunner
GPUModelRunner.execute_model??

Line by Line 

# torch.inference_mode() :)

ok looks like execute_model
depends on 
GPUModelRunner.execute_model(
    self,
    scheduler_output: 'SchedulerOutput',
    intermediate_tensors: vllm.sequence.IntermediateTensors | None = None,
) -> vllm.v1.outputs.ModelRunnerOutput | vllm.sequence.IntermediateTensors | None

# torch.inference_mode() :)

# that looks funny
so looks like is
self.execute_model_state
for keeping track of worker current status

if self.execute_model_state is not None:
    raise RuntimeError(
        "State error: sample_tokens() must be called "
        "after execute_model() returns None."
    )


# ok that's interesting so num_spec_tokens is responsible for 
speculative decoding

# Shape: [batch_size, num_spec_tokens]
# Example with 2 requests, 5 spec tokens each:
_draft_token_ids = [
    [1042, 887, 2311, 459, 1033],  # request 0's guesses
    [512, 1887, 943, 2841, 118],   # request 1's guesses
]
The edge case: num_spec_tokens > 0 means spec decoding is configured, 
but _draft_token_ids is None means no drafts were produced 
this iteration (drafter couldn't run — 
cold start or input too long for draft model's context window).
The scheduler output needs modification to reflect "no speculation this step" 
but with async scheduling that output lives in another process, 
hence the defensive copy.

# ok so proplem later it calls 
GPUModelRunner._update_states?? # Todo: Come back to this function crucial!

Source:   
    def _update_states(self, scheduler_output: "SchedulerOutput") -> None:
        """Update the cached states and the persistent batch with the scheduler
        output.

        The updated states are used by the `_prepare_inputs` function to create
        the input GPU tensors for the model.

        The SamplingMetadata is updated and copied to the GPU if there is a
        new/resumed/paused/finished request in the batch.
        """

# there are no draft tokens with async scheduling,
# we clear the spec_decoding info in scheduler_output and
# use normal sampling but rejection_sampling.
if self.use_async_scheduling:
    req_state.prev_num_draft_len = num_spec_tokens
    if num_spec_tokens and self._draft_token_ids is None:
        scheduler_output.total_num_scheduled_tokens -= num_spec_tokens
        scheduler_output.num_scheduled_tokens[req_id] -= num_spec_tokens
        scheduler_output.scheduled_spec_decode_tokens.pop(req_id, None)


 # self._draft_token_ids is None when `input_fits_in_drafter=False`
        # and there is no draft tokens scheduled. so it need to update the
        # spec_decoding info in scheduler_output with async_scheduling.
        # use deepcopy to avoid the modification has influence on the
        # scheduler_output in engine core process.
        # TODO(Ronald1995): deepcopy is expensive when there is a large
        # number of requests, optimize it later.
        if (
            self.use_async_scheduling
            and self.num_spec_tokens
            and self._draft_token_ids is None
        ):
            scheduler_output = deepcopy(scheduler_output)


num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
num_scheduled_tokens — The scheduler has already decided which requests 
to batch together. This grabs the total token count 
(sum of all prefill tokens + one decode token per active sequence) 
that the model will process in this iteration.

# This is interesting like execute_model
doesn't know if it execxutes in prefil or decode mode 
it just knows how many tokens scheduler asked 

# Inteligence lives inside of scheduler 


Most important things happens here
from vllm.v1.worker.gpu_model_runner import GPUModelRunner
GPUModelRunner.execute_model??

# this line is a heart of state managment
# need to look separately
self._update_states(scheduler_output)

# ok next 
num_reqs = self.input_batch.num_reqs
req_ids = self.input_batch.req_ids
tokens = [scheduler_output.num_scheduled_tokens[i] for i in req_ids]
num_scheduled_tokens_np = np.array(tokens, dtype=np.int32)
max_num_scheduled_tokens = int(num_scheduled_tokens_np.max())
num_tokens_unpadded = scheduler_output.total_num_scheduled_tokens


# decipher
num_reqs = self.input_batch.num_reqs
How many requests in the batch. Say `3`.
req_ids = self.input_batch.req_ids

List of request UUIDs:
["cmpl-a1b2c3...", "cmpl-d4e5f6...", "cmpl-g7h8i9..."]

tokens = [scheduler_output.num_scheduled_tokens[i] for i in req_ids]
scheduler_output.num_scheduled_tokens` is a dict:
{
    "cmpl-a1b2c3...": 512,
    "cmpl-d4e5f6...": 1,
    "cmpl-g7h8i9...": 64,
}

The list comprehension looks up each request ID → gets `[512, 1, 64]`
num_scheduled_tokens_np = np.array(tokens, dtype=np.int32)
Convert to numpy: `array([512, 1, 64], dtype=int32)`
max_num_scheduled_tokens = int(num_scheduled_tokens_np.max())
Longest sequence: `512`
Used for padding decisions and attention mask shapes.
num_tokens_unpadded = scheduler_output.total_num_scheduled_tokens

Total tokens: `577` (512 + 1 + 64)
**Summary:**

`num_reqs` | 3 | batch size (requests) 
`req_ids` | ["cmpl-a1...", ...] | which requests
`num_scheduled_tokens_np` | [512, 1, 64] | tokens per request 
`max_num_scheduled_tokens` | 512 | longest one 
`num_tokens_unpadded` | 577 | total work 

# ok so next tuple unpacking 
(logits_indices,spec_decode_metadata,
) = self._prepare_inputs(
    scheduler_output,
    num_scheduled_tokens_np,
)

# ok looks like we need to look into self._prepare_inputs()

from vllm.v1.worker.gpu_model_runner import GPUModelRunner
GPUModelRunner._prepare_inputs??
